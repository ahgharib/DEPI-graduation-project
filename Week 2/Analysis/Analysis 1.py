# -*- coding: utf-8 -*-
"""Python Script for DEPI project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qQ5FPF5v2-gbJgGI6N8M1ffubb57qqGZ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

file_path = '/content/products.csv'
df = pd.read_csv(file_path)

print("First few rows of the dataset:")
print(df.head())

from google.colab import drive
drive.mount('/content/drive')

print("\nDataset info:")
df.info()

print("\nMissing values in each column:")
print(df.isnull().sum())

print("\nNumber of duplicate rows:", df.duplicated().sum())
df_cleaned = df.drop_duplicates()

threshold = len(df_cleaned) * 0.5
df_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)

for column in df_cleaned.select_dtypes(include=['float64', 'int64']).columns:
    df_cleaned[column] = df_cleaned[column].fillna(df_cleaned[column].mean())

for column in df_cleaned.select_dtypes(include=['object']).columns:
    if df_cleaned[column].mode().size > 0:
        mode_value = df_cleaned[column].mode()[0]
        df_cleaned[column] = df_cleaned[column].fillna(mode_value)

z_scores = np.abs(stats.zscore(df_cleaned.select_dtypes(include=['float64', 'int64'])))
outliers = (z_scores > 3).any(axis=1)
print(f"\nNumber of outliers detected: {outliers.sum()}")

df_cleaned = df_cleaned[~outliers]

from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for column in df_cleaned.select_dtypes(include=['object']):
    le = LabelEncoder()
    df_cleaned[column] = le.fit_transform(df_cleaned[column])
    label_encoders[column] = le

print("\nMissing values after preprocessing:")
print(df_cleaned.isnull().sum())

print("\nNumber of duplicates after preprocessing:", df_cleaned.duplicated().sum())

print("\nStatistical summary after cleaning:")
print(df_cleaned.describe())

plt.figure(figsize=(12, 8))
sns.heatmap(df_cleaned.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap of Numerical Features")
plt.show()

if 'CategoryColumn' in df_cleaned.columns:
    plt.figure(figsize=(8, 5))
    sns.countplot(data=df_cleaned, x='CategoryColumn', palette='viridis')
    plt.title("Distribution of CategoryColumn")
    plt.xticks(rotation=45)
    plt.show()

for column in df_cleaned.select_dtypes(include=['float64', 'int64']):
    plt.figure(figsize=(8, 5))
    sns.boxplot(x=df_cleaned[column])
    plt.title(f"Boxplot for {column}")
    plt.show()

sns.pairplot(df_cleaned.select_dtypes(include=['float64', 'int64']))
plt.show()

cleaned_file_path = '/content/products done.csv'
df_cleaned.to_csv(cleaned_file_path, index=False)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

file_path = '/content/Customer.csv'
df = pd.read_csv(file_path)

print("First few rows of the dataset:")
print(df.head())

print("\nDataset info:")
df.info()

print("\nMissing values in each column:")
print(df.isnull().sum())

print("\nNumber of duplicate rows before cleaning:", df.duplicated().sum())
df_cleaned = df.drop_duplicates()
print("Number of duplicate rows after cleaning:", df_cleaned.duplicated().sum())

threshold = len(df_cleaned) * 0.5
df_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)

for column in df_cleaned.select_dtypes(include=['float64', 'int64']).columns:
    df_cleaned[column] = df_cleaned[column].fillna(df_cleaned[column].mean())

for column in df_cleaned.select_dtypes(include=['object']).columns:
    if df_cleaned[column].mode().size > 0:
        df_cleaned[column] = df_cleaned[column].fillna(df_cleaned[column].mode()[0])

z_scores = np.abs(stats.zscore(df_cleaned.select_dtypes(include=['float64', 'int64'])))
outliers = (z_scores > 3).any(axis=1)
print(f"\nNumber of outliers detected: {outliers.sum()}")

df_cleaned = df_cleaned[~outliers]

from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for column in df_cleaned.select_dtypes(include=['object']):
    le = LabelEncoder()
    df_cleaned[column] = le.fit_transform(df_cleaned[column])
    label_encoders[column] = le

print("\nMissing values after preprocessing:")
print(df_cleaned.isnull().sum())

print("\nNumber of duplicates after preprocessing:", df_cleaned.duplicated().sum())

plt.figure(figsize=(12, 8))
sns.heatmap(df_cleaned.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap of Numerical Features")
plt.show()

if 'CategoryColumn' in df_cleaned.columns:
    plt.figure(figsize=(8, 5))
    sns.countplot(data=df_cleaned, x='CategoryColumn', palette='viridis')
    plt.title("Distribution of CategoryColumn")
    plt.xticks(rotation=45)
    plt.show()

for column in df_cleaned.select_dtypes(include=['float64', 'int64']):
    plt.figure(figsize=(8, 5))
    sns.boxplot(x=df_cleaned[column])
    plt.title(f"Boxplot for {column}")
    plt.show()

sns.pairplot(df_cleaned.select_dtypes(include=['float64', 'int64']))
plt.show()

cleaned_file_path = '/content/customer done.csv'
df_cleaned.to_csv(cleaned_file_path, index=False)

print(f"\nCleaned data saved to: {cleaned_file_path}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder

file_path = '/content/order_details.csv'
df = pd.read_csv(file_path)

print("First few rows of the dataset:")
print(df.head())

print("\nDataset info:")
df.info()

print("\nNumber of duplicate rows before cleaning:", df.duplicated().sum())
df_cleaned = df.drop_duplicates()
print("Number of duplicate rows after cleaning:", df_cleaned.duplicated().sum())

print("\nMissing values in each column:")
print(df_cleaned.isnull().sum())

threshold = len(df_cleaned) * 0.5
df_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)

for column in df_cleaned.select_dtypes(include=['float64', 'int64']).columns:
    mean_value = df_cleaned[column].mean()
    df_cleaned[column] = df_cleaned[column].fillna(mean_value)

for column in df_cleaned.select_dtypes(include=['object']).columns:
    if not df_cleaned[column].mode().empty:  # Check if the mode exists
        mode_value = df_cleaned[column].mode()[0]  # Get the first mode value
        df_cleaned[column] = df_cleaned[column].fillna(mode_value)  # Fill NaN with mode

z_scores = np.abs(stats.zscore(df_cleaned.select_dtypes(include=['float64', 'int64'])))
outliers_zscore = (z_scores > 3).any(axis=1)
print(f"\nNumber of outliers detected using Z-score: {outliers_zscore.sum()}")

df_cleaned_zscore = df_cleaned[~outliers_zscore]

Q1 = df_cleaned.select_dtypes(include=['float64', 'int64']).quantile(0.25)
Q3 = df_cleaned.select_dtypes(include=['float64', 'int64']).quantile(0.75)
IQR = Q3 - Q1
outliers_iqr = ((df_cleaned.select_dtypes(include=['float64', 'int64']) < (Q1 - 1.5 * IQR)) |
               (df_cleaned.select_dtypes(include=['float64', 'int64']) > (Q3 + 1.5 * IQR))).any(axis=1)
print(f"\nNumber of outliers detected using IQR: {outliers_iqr.sum()}")

df_cleaned_iqr = df_cleaned[~outliers_iqr]

scaler = StandardScaler()  # Optionally use MinMaxScaler() for normalization
df_cleaned_scaled = df_cleaned.copy()
df_cleaned_scaled[df_cleaned.select_dtypes(include=['float64', 'int64']).columns] = scaler.fit_transform(df_cleaned.select_dtypes(include=['float64', 'int64']))

df_cleaned_encoded = pd.get_dummies(df_cleaned, drop_first=True)

print("\nSummary statistics after cleaning:")
print(df_cleaned.describe())

plt.figure(figsize=(12, 8))
sns.heatmap(df_cleaned.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap of Numerical Features")
plt.show()

for column in df_cleaned.select_dtypes(include=['float64', 'int64']):
    plt.figure(figsize=(8, 5))
    sns.boxplot(x=df_cleaned[column])
    plt.title(f"Boxplot for {column}")
    plt.show()

if 'CategoryColumn' in df_cleaned.columns:
    plt.figure(figsize=(8, 5))
    sns.countplot(data=df_cleaned, x='CategoryColumn', palette='viridis')
    plt.title("Distribution of CategoryColumn")
    plt.xticks(rotation=45)
    plt.show()

sns.pairplot(df_cleaned.select_dtypes(include=['float64', 'int64']))
plt.show()

cleaned_file_path = '/content/order_details done.csv'
df_cleaned.to_csv(cleaned_file_path, index=False)

print(f"\nCleaned data saved to: {cleaned_file_path}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler

# Load the CSV file
file_path = '/content/Store_edit.csv'  # Replace with the path to your file
data = pd.read_csv(file_path)

# Display the first few rows
print("First few rows of the data:")
print(data.head())

# 1. Remove Duplicates
print("\nNumber of duplicate rows before removing:", data.duplicated().sum())
data = data.drop_duplicates()
print("Number of duplicate rows after removing:", data.duplicated().sum())

# 2. Handle Missing Values
print("\nMissing values before imputation:")
print(data.isnull().sum())

# Fill missing values (numerical columns) with the median
data.fillna(data.median(numeric_only=True), inplace=True)

for col in data.select_dtypes(include='object').columns:
    if not data[col].mode().empty:
        mode_value = data[col].mode()[0]
        data[col] = data[col].fillna(mode_value)

print("Missing values after imputation:")
print(data.isnull().sum())

def detect_outliers(df, threshold=3):
    z_scores = np.abs(stats.zscore(df.select_dtypes(include=np.number)))
    outliers = (z_scores > threshold)
    return np.where(outliers)

outliers = detect_outliers(data)
print("\nOutliers found at indices:", outliers)

# You can remove outliers based on the index
data_cleaned = data[(np.abs(stats.zscore(data.select_dtypes(include=np.number))) < 3).all(axis=1)]
print("Shape of data after removing outliers:", data_cleaned.shape)

# Assuming 'numerical_columns' is a list of the numerical column names
data_cleaned.loc[:, numerical_columns] = scaler.fit_transform(data_cleaned[numerical_columns])

plt.figure(figsize=(10, 6))
data.hist(bins=30, figsize=(20, 10), color='blue', edgecolor='black')
plt.suptitle('Distribution of Numerical Features (Before Cleaning)', fontsize=16)
plt.show()

plt.figure(figsize=(10, 6))
data_cleaned.hist(bins=30, figsize=(20, 10), color='green', edgecolor='black')
plt.suptitle('Distribution of Numerical Features (After Cleaning)', fontsize=16)
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(data=data_cleaned)
plt.title('Boxplot of Numerical Features')
plt.xticks(rotation=90)
plt.show()

sns.pairplot(data_cleaned)
plt.title("Pairplot of Numerical Features")
plt.show()

output_path = 'Processed_Store_Data done.csv'
data_cleaned.to_csv(output_path, index=False)
print(f"Data cleaning complete. Cleaned data saved to '{output_path}'.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler

file_path = '/content/sales.csv'
data = pd.read_csv(file_path)

print("First few rows of the dataset:")
print(data.head())

print("\nDataset Info:")
print(data.info())

print("\nStatistical summary of the dataset:")
print(data.describe())

print("\nNumber of duplicate rows before removing:", data.duplicated().sum())
data = data.drop_duplicates()
print("Number of duplicate rows after removing:", data.duplicated().sum())

print("\nMissing values before imputation:")
print(data.isnull().sum())

data.fillna(data.median(numeric_only=True), inplace=True)

for col in data.select_dtypes(include='object').columns:
    mode_value = data[col].mode()[0]  # Get the mode of the column
    data[col] = data[col].fillna(mode_value)  # Assign the filled column back to the DataFrame

print("Missing values after imputation:")
print(data.isnull().sum())

def detect_outliers(df, threshold=3):
    z_scores = np.abs(stats.zscore(df.select_dtypes(include=np.number)))
    outliers = (z_scores > threshold)
    return np.where(outliers)

outliers = detect_outliers(data)
print("\nOutliers found at indices:", outliers)

data_cleaned = data[(np.abs(stats.zscore(data.select_dtypes(include=np.number))) < 3).all(axis=1)]
print("Shape of data after removing outliers:", data_cleaned.shape)

scaler = StandardScaler()
numerical_columns = data_cleaned.select_dtypes(include=np.number).columns
data_cleaned[numerical_columns] = scaler.fit_transform(data_cleaned[numerical_columns])

plt.figure(figsize=(10, 6))
data.hist(bins=30, figsize=(20, 10), color='blue', edgecolor='black')
plt.suptitle('Distribution of Numerical Features (Before Cleaning)', fontsize=16)
plt.show()

plt.figure(figsize=(10, 6))
data_cleaned.hist(bins=30, figsize=(20, 10), color='green', edgecolor='black')
plt.suptitle('Distribution of Numerical Features (After Cleaning)', fontsize=16)
plt.show()

plt.figure(figsize=(12, 8))
corr_matrix = pd.DataFrame(data_cleaned, columns=numerical_columns).corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(data=pd.DataFrame(data_cleaned, columns=numerical_columns))
plt.title('Boxplot of Numerical Features')
plt.xticks(rotation=90)
plt.show()

sns.pairplot(pd.DataFrame(data_cleaned, columns=numerical_columns))
plt.title("Pairplot of Numerical Features")
plt.show()

output_path = 'sales_Data done.csv'
pd.DataFrame(data_cleaned, columns=data.columns).to_csv(output_path, index=False)
print(f"Data cleaning complete. Cleaned data saved to '{output_path}'.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler

file_path = '/content/final_Date.csv'  # Path to your file
data = pd.read_csv(file_path)

print("First few rows of the dataset:")
print(data.head())

print("\nDataset Info:")
print(data.info())

print("\nStatistical summary of the dataset:")
print(data.describe())

print("\nNumber of duplicate rows before removing:", data.duplicated().sum())
data = data.drop_duplicates()
print("Number of duplicate rows after removing:", data.duplicated().sum())

print("\nMissing values before handling:")
print(data.isnull().sum())

data.fillna(data.median(numeric_only=True), inplace=True)

for col in data.select_dtypes(include='object').columns:
    data[col] = data[col].fillna(data[col].mode()[0])

print("Missing values after handling:")
print(data.isnull().sum())

def detect_outliers(df, threshold=3):
    z_scores = np.abs(stats.zscore(df.select_dtypes(include=np.number)))
    outliers = (z_scores > threshold)
    return np.where(outliers)

outliers = detect_outliers(data)
print("\nOutliers found at indices:", outliers)

data_cleaned = data[(np.abs(stats.zscore(data.select_dtypes(include=np.number))) < 3).all(axis=1)]
print("Shape of data after removing outliers:", data_cleaned.shape)

plt.figure(figsize=(10, 6))
data.hist(bins=30, figsize=(20, 10), color='blue', edgecolor='black')
plt.suptitle('Distribution of Numerical Features (Before Cleaning)', fontsize=16)
plt.show()

output_path = 'Processed_final_Date done.csv'
pd.DataFrame(data_cleaned, columns=data.columns).to_csv(output_path, index=False)
print(f"Data cleaning complete. Cleaned data saved to '{output_path}'.")

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from scipy import stats

file_path = '/content/Feedback_cleaned2.csv'
df = pd.read_csv(file_path)

print("First few rows of the dataset:")
print(df.head())

print("\nDataset info:")
df.info()

print("\nMissing values in each column:")
print(df.isnull().sum())

print("\nNumber of duplicate rows before removal:", df.duplicated().sum())
df_cleaned = df.drop_duplicates()
print("\nNumber of duplicate rows after removal:", df_cleaned.duplicated().sum())

threshold = len(df_cleaned) * 0.5
df_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)

for column in df_cleaned.select_dtypes(include=['float64', 'int64']).columns:
    df_cleaned[column] = df_cleaned[column].fillna(df_cleaned[column].mean())

for column in df_cleaned.select_dtypes(include=['object']).columns:
    df_cleaned[column] = df_cleaned[column].fillna(df_cleaned[column].mode()[0])

z_scores = np.abs(stats.zscore(df_cleaned.select_dtypes(include=['float64', 'int64'])))
outliers = (z_scores > 3).any(axis=1)
print(f"\nNumber of outliers detected: {outliers.sum()}")

df_cleaned = df_cleaned[~outliers]

label_encoders = {}
for column in df_cleaned.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df_cleaned[column] = le.fit_transform(df_cleaned[column])
    label_encoders[column] = le

print("\nMissing values after preprocessing:")
print(df_cleaned.isnull().sum())

print("\nNumber of duplicates after preprocessing:", df_cleaned.duplicated().sum())

print("\nStatistical summary after cleaning:")
print(df_cleaned.describe())

cleaned_file_path = '/content/cleaned_feedback done.csv'
df_cleaned.to_csv(cleaned_file_path, index=False)

print(f"\nCleaned data saved to: {cleaned_file_path}")

